{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Datos (II): extrayendo vocabulario para clasificar textos\n",
    "\n",
    "En esta práctica vamos a poner en práctica todo lo aprendido con pandas, numpy y hasta con expresiones regulares. El objetivo de la práctica es tratar de extraer un conjunto de palabras (vocabulario) que nos permitan clasificar (de una manera muy sencilla) si una crítica o review de una película es positiva o negativa sin siquiera leer el texto. ¿Cómo lo vamos a hacer? La idea detrás de este ejemplo es la siguiente: vamos a ver qué palabras son las más comunes en un conjunto de comentarios positivos; vamos a ver qué palabras son las más comunes es un conjunto de comentarios negativos; y, dado un nuevo comentario, vamos a decidir si el conjunto de palabras que lo forman se parece más al conjunto de palabras \"positiva\" o al de palabras \"negativas\". \n",
    "\n",
    "Para ello, os proponemos un conjunto de 5000 críticas de películas del portal imdb. Cada una de estas críticas está previamente catalogada como positiva (sentiment = 1) o negativa (sentiment = 0). El hecho de que tengamos conocimiento de qué tipo de crítica es (positiva o negativa) nos va a permitir crear un pequeño y simple clasificador de críticas que luego podremos evaluar. \n",
    "\n",
    "Los pasos que vamos a dar son los siguientes:\n",
    "- Importar el conjunto de críticas en un DataFrame. \n",
    "- \"Limpiar\" los comentarios para tratar de dejar todos ellos en un mismo formato: todo en minúsculas, sin caracteres especiales, etc.\n",
    "- Separar el conjunto de críticas en un conjunto de entrenamiento (alrededor del 80% de las muestras) y un conjunto de test (alrededor del 20% de las muestras)\n",
    "- Extrar dos diccionarios de palabras: un diccionario de palabras \"positivas\" y un diccionario de palabras \"negativas\". A partir de estos diccionarios crearemos un DataFrame vocabulario que nos ayudará a clasificar nuevas muestras\n",
    "- A partir de un comentario de test, extraeremos el diccionario de dicho comentario, lo \"combinaremos\" con el vocabulario existente de entrenamiento y veremos si tiene más palabras \"positivas\" o \"negativas\". En función del número de palabras lo clasificaremos como comentario positivo o comentario negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a crar el DataFrame comentarios. Echa un vistazo al número de elementos y columnas que tiene, cuáles son las 5 primeras filas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comentarios = pd.read_csv('comentarios.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuenta el número de comentarios positivos y negativos. ¿Podemos decir que el número de críticas está \"balanceado\" (hay un número parecido de comentraios positivos y negativos)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos\n",
    "\n",
    "Vamos a comenzar con una pequeña limpieza de los datos para crear un conjunto de palabras lo suficientemente bueno. Principalmente vamos a:\n",
    "- eliminar cualquier cosa que no sea una letra mayúscula o minúscula (sustituiremos cada caracter especial por un espacio en blanco\n",
    "- pasar todo a minúsculas\n",
    "- dividir el texto en una lista de palabras\n",
    "- eliminar aquellas palabras de longitud  cero\n",
    "- eliminar aquellas palabras que forman parte de la lista de stop_words: palabras que, por la cantidad de veces que se usan, no aportan nada de informacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar esta limpieza, crea una función procesa_comentario que reciba como parámetro de entrada un comentario y devuelva un listado de palabras \"limpias\" siguiendo los pasos escritos anteriormente. Para ello puedes utilizar la siguiente información:\n",
    "\n",
    "- Para eliminar cualquier cosa que no sea una letra, utiliza la función re.sub. Esta función reciba 3 parámetros de entrada: la expresión regular que tiene que buscar para eliminar; la cadena de caracteres por las que va a eliminar cada match de la expresión regular; y el string donde va a trabajar la función. \n",
    "- Para pasar todo a minúsculas existe la función lower\n",
    "- Para dividir un texto en un listado de palabras existe la función split\n",
    "- Para eliminar palabras de una lista puedes utilizar la función filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expresion_regular = re.compile('[^      ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lista_stop = ['a','about','above','after','again','against','all','am','an','and','any','are','arent','as','at','be','because','been','before','being',\n",
    "'below','between','both','but','by','cant','cannot','could','couldnt','did','didnt','do','does','doesnt','doing','dont','down','during','each',\n",
    "'few','for','from','further','had','hadnt','has','hasnt','have','havent','having','he','hed','hell','hes','her','here',\n",
    "'heres','hers','herself','him','himself','his','how','hows','i','id','ill','im','ive','if','in','into','is','isnt','it',\n",
    "'its','its','itself','lets','me','more','most','mustnt','my','myself','no','nor','not','of','off','on','once','only','or',\n",
    "'other','ought','our','ours','ourselves','out','over','own','same','shant','she','shed','shell','shes','should','shouldnt',\n",
    "'so','some','such','than','that','thats','the','their','theirs','them','themselves','then','there','theres','these','they','theyd',\n",
    "'theyll','theyre','theyve','this','thos','through','to','too','under','until','up','very','was','wasnt','we','wed','well',\n",
    "'were','weve','were','werent','what','whats','when','whens','where','wheres','which','while','who','whos','whom','why',\n",
    "'whys','with','wont','would','wouldnt','you','youd','youll','youre','youve','your','yours','yourself','yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def procesa_comentario(comentario):\n",
    "    comentario = re.sub( ##Rellenar aquí con la expresión regular, el caracter de sustitución y el texto a procesar\n",
    "    comentario = ##Pasar el comentario a minúsculas\n",
    "    comentario = ##Dividir el string en una lista de palabras\n",
    "    comentario = #Eliminar las de longitud 0\n",
    "    comentario = #Eliminar aquellas que pertenezcan a la lista_stop\n",
    "    return comentario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definida la función procesa_comentario, modifica el DataFrame comentarios de forma que la columna review deje de ser un texto para convertirse en un listado de palabras. ¿Puedes hacer esto sin estructuras iterativas? Piensa en la función apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios['review'] = ## Aplicar la función procesa_comentario\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División del DataFrame en train y set\n",
    "\n",
    "Vamos a dividir nuestra DataFrame en dos DataFrames de entrenamiento (4000 comentarios) y test (1000 comentarios). Para ello, vamos a seleccionar 4000 índices aleatorios entre 1 y 5000. Para esto, vamos a utilizar la función np.random.choice, que elige 4000 números entre 0 y 4999 sin repetición. A partir de dicho conjunto de índices, crea el conjunto índices test y divide el DataFrame comentarios en comentarios_train y comentarios_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "indices_train =  np.random.choice(5000, 4000, replace = False)\n",
    "indices_test = ##Calcula\n",
    "comentarios_train = #Selecciona 4000 comentarios de entrenamiento\n",
    "comentarios_test =  #Selecciona 1000 comentarios de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de diccionarios\n",
    "\n",
    "Vamos a crear dos diccionarios: diccionario_negativo y diccionario_positivo. Cada diccionario se creará a partir de los comentarios negativos y positivos de entrenamiento, respectivamente. El diccionario deberá contener el conjunto de palabras que aparecen en todos los comentarios (como clave) y el número de veces que aparecen (como valor). Recuerda la utilidad de la función get asociada a un diccionario y la función iterrows que te permite ir acciendo a cada fila del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_negativo = {}\n",
    "diccionario_positivo = {}\n",
    "for indice, fila in comentarios_train.iterrows():\n",
    "    #Ahora índice es el índice de la fila (no nos interesa demsasiado) y fila es la serie que contieen los datos: fila['id'], fila['sentiment'] y fila['review']\n",
    "    #Recuerda que puedes hacer diccionario_negativo[palabra] = diccionario_negativo.get(palabra, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez construidos los diccionarios vamos a convertilos cada uno de ellos en una Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_negativo = pd.Series(diccionario_negativo, name='negativo')\n",
    "serie_positivo = pd.Series(diccionario_positivo, name='positivo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analiza las dos series utilizando la función describe. Observa que, por ejemplo, las dos series tienen más de 24000 palabras y que la media de veces que cada palabra ronda e 9.4 y el 9.5. Sin embargo, la desviación típica es muy alta (en torno a 40) y el valor de cuartil 3 es muy bajo (5): esto indica que hay muchas palabras que aparecen muy pocas veces. \n",
    "\n",
    "Para reducir el número de apariciones, vamos a eliminar de cada serie aquellas palabras que no aparezcan tantas veces como la media actual de apariciones. Observa que ahora nos quedamos con unas 3600 y 4000 palabras, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print serie_negativo.describe()\n",
    "print serie_positivo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print serie_negativo.describe()\n",
    "print serie_positivo.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como último pasa para crear el vocabulario, vamos a combinar las dos series en un DataFrame que contenga como índices las palabras de los dos vocabularios y 2 columnas: negativo y positivo; en las que aparezca el número de veces que dichas palabras aparecen en cada tipo de comentario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = pd.concat([serie_negativo, serie_positivo], axis = 1)\n",
    "vocabulario = vocabulario.fillna(0)\n",
    "vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra por pantalla las 10 palabras \"negativas\" que más veces aparecen y las 10 palabras \"positivas\" que más veces aparecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a transformar la información cuantitativa (cuántas veces aparece cada palabra) en un porcentaje relativo. Es decir, dividir cada fila del DataFrame entre la suma del número de veces que dicha palabra aparece en comentarios \"positivos\" y \"negativos\". De esta manera tenemos algo parecido a la \"probabilidad\" de que dicha palabra aparezca en un comentario \"positivo\" en uno \"negativo\". Puedes utilizar la función apply aplicada a cada fila. Así, si la palabra \"ejemplo\" aparece 5 veces en un comentario negativo y 15 en uno positivo, se transformará en 0.25 (5/20) y 0.75 (15/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = vocabulario.apply( #Rellenar\n",
    "vocabulario.fillna(0)\n",
    "vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación de nuevos comentarios\n",
    "\n",
    "A partir del DataFrame comentarios_test vamos a hacer lo siguiente. Para cada comentario de test:\n",
    "- obtener diccionario_test con las palabras de dicho comentario junto con el número de veces que aparece\n",
    "- transformar dicho diccionario en una Series\n",
    "- multiplicar la serie_test por cada columna del DataFrame. De esta manera, cada palabra que aparezca en serie_test se multiplicará por la probabilidad de ser positivo o negativo. \n",
    "- Sumar por columnas el resultado de la multiplicación anterior: esto nos dará un resultado final de probabilidades acumulada\n",
    "- Obtener el nombre de la columna que mayor suma ha obtenido: si es la columna \"negativo\" entonces predecimos que dicho comentario es de tipo sentiment=0; al contrario predeciremos sentiment=1;\n",
    "- Contar el número de acierots/fallos de nuestro clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_aciertos = 0\n",
    "num_fallos = 0\n",
    "for indice, fila in comentarios_test.iterrows():\n",
    "    diccionario_test = {}\n",
    "    for ##\n",
    "    serie_test = pd.Series(diccionario_test)\n",
    "    \n",
    "    #Recuerda que para multiplicar una serie y un dataframe puedes usuar \n",
    "    #vocabulario.multiply(serie_test, axis=0) prediccion = vocabulario.add(serie_test, axis=0).sum().idxmax()\n",
    "    \n",
    "    #La funcion idxmax() de una Series/DataFrame te devuelve el nombre del índice que tenga mayor valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (num_aciertos)\n",
    "print (num_fallos)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py37machlearn]",
   "language": "python",
   "name": "conda-env-py37machlearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
