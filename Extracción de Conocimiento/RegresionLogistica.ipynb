{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f14ae205e4eddb84aa79c7e72b6af04",
     "grade": false,
     "grade_id": "cell-19e5551f46b17f23",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Práctica 3 - Regresión logística\n",
    "\n",
    "- [Regresión logística](#Regresión-logística)\n",
    "- [Regresión logística con regularización](#Regresión-logística-con-regularización)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e42169c318803d895ac327130afabd5",
     "grade": false,
     "grade_id": "cell-2fe73488776153ba",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Importamos todas las librerías que vamos a utilizar durante la segunda práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28e2b9d76bd67c03f69415b5a7c3bf2f",
     "grade": false,
     "grade_id": "cell-a23b75980fbad3ae",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# %load ../../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    " \n",
    "#%config InlineBackend.figure_formats = {'pdf',}\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "573c68264f25ac36c2b38fe5ee867140",
     "grade": false,
     "grade_id": "cell-07704a1bac440c23",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Definimos una función que lee un archivo que contiene los datos de entrenamiento. La función recibe como parámetro de entrada el nombre del fichero que contiene los datos de entrenamiento así como el símbolo utilizado para separar los valores de las variables del problema. La función muestra las dimensiones del dataset así como los 5 primeros ejemplos del mismo.\n",
    "\n",
    "Debéis llamar a la función loadtxt de numpy con los parámetros apropiados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c6798931a785cefa338f75059c815ab",
     "grade": false,
     "grade_id": "cell-5236a224e35865d8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def lecturaDatos(nombreFichero, simboloDelimitador):\n",
    "#     datos = #<RELLENAR>\n",
    "\n",
    "\n",
    "    print('Dimensiones de los datos: ',datos.shape)\n",
    "    print(datos[1:6,:])\n",
    "    return(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6aae424f2ce19a269f98c4182517ae32",
     "grade": false,
     "grade_id": "cell-542893f0e7ca6e86",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Definimos una función que muestre los datos de entrenamiento gráficamente. \n",
    "\n",
    "Esta función va a ser genérica para problemas de clasificación de dos clases por lo que\n",
    "* La clase negativa (no admitido) va a ser la que tenga asociada la etiqueta 0\n",
    "* La clase positiva (admitido) va a ser la que tenga asociada la etiqueta 1\n",
    "\n",
    "Además asumimos que la variable de salida (clase) será la última.\n",
    "\n",
    "La función recibe como parámetros de entrada los datos, las etiquetas de los ejes x e y (dos variables de entrada) así como la etiqueta para los datos la clase positiva y de la clase negativa (se muestran en la leyenda de la figura)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b1ae4e452aae247e8b170011f4cd511",
     "grade": false,
     "grade_id": "cell-9fa20246bd923826",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def muestraGrafica(datos, etiqueta_ejeX, etiqueta_ejeY, etiqueta_Positiva, etiqueta_Negativa, axes=None):\n",
    "    # La variable claseNegativa debe ser un array de booleanos compuesto por tantos elementos como ejemplos\n",
    "        # Aquellos alumnos no admitidos (clase 0) deben tener True en su correspondiente posicion del array de booleanos y False en el resto\n",
    "    claseNegativa = datos[:,-1] == 0\n",
    "    # La variable clasePositiva debe ser un array de booleanos compuesto por tantos elementos como ejemplos\n",
    "        # Aquellos alumnos admitidos (clase 1) deben tener True en su correspondiente posicion del array de booleanos y False en el resto\n",
    "#     clasePositiva = #<RELLENAR>\n",
    "\n",
    "\n",
    "    \n",
    "    # Si no se pasa un manejador de una figura se crea una\n",
    "    if axes == None:\n",
    "        axes = plt.gca()\n",
    "    # Se muestra la nube de puntos: + negros para la clase 1 (admitidos) y circulos amarillos para la clase 0 (no admitidos)\n",
    "#     axes.scatter(#<RELLENAR>, #<RELLENAR>, marker='+', c='k', s=60, linewidth=2, label=etiqueta_Positiva)\n",
    "#     axes.scatter(#<RELLENAR>, #<RELLENAR>, c='y', s=60, label=etiqueta_Negativa)\n",
    "\n",
    "\n",
    "    axes.set_xlabel(etiqueta_ejeX)\n",
    "    axes.set_ylabel(etiqueta_ejeY)\n",
    "    axes.legend(frameon= True, fancybox = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1e3a2b47c9b0a6f4c872c1b0adfcaba",
     "grade": false,
     "grade_id": "cell-fa24f77403985c9b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40001704f569d48adacc0355fc404708",
     "grade": false,
     "grade_id": "cell-5484c905e3952eb6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Se leen los datos utilizando la función definida anteriormente. El nombre del fichero a leer es ex3data1.txt y el delimitador de los valores de las variables es la coma (,).\n",
    "\n",
    "Los datos contenidos en este fichero corresponden a un problema en el que se debe predecir si un estudiante es admitido en una universidad o no. El acceso a la universidad viene determinado por la nota obtenida por los estudiantes en dos exámenes realizados previamente. El archivo contiene el histórico de las notas obtenidas en ambos exámenes (variables de entrada) por 100 alumnos así como si han ingresado a la universidad o no (variable de salida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65300c99245d7907d4617f9197903c39",
     "grade": false,
     "grade_id": "cell-22f6fa8af6791ebe",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# datos = #<RELLENAR>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63007d73a8a84e5ecad9522ee3663491",
     "grade": false,
     "grade_id": "cell-b25a60f048a5dc3f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "A continuación se crean los datos de entrada (X) y de salida (y) para poder realizar el aprendizaje de los parámetros de la regresión logística ($\\theta$). Recordar que la primera columna de los datos de entrada debe estar compuesta por unos para multiplicar al término independiente de la regresión logística. Por tanto, la variable X estará compuesta por tantas columnas como variables de entrada más una (la de unos). En este caso serán 3 columnas. La variable y debe contier la información de si los alumnos han sido admitidos o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33eeebffa6ba2c26476182871bb07eef",
     "grade": false,
     "grade_id": "cell-0ed9a3ad8c08868d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# X = #<RELLENAR>\n",
    "# y = #<RELLENAR>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd53dfdbdb7d153b43c3aafff205623d",
     "grade": false,
     "grade_id": "cell-5a4c9599eb8bdefa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Realizar la llamada a la función que muestra los ejemplos utilizando los datos recién leídos y las etiquetas apropiadas. Los alumnos admitidos se codifican con 1 y los no admitidos con 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d69e2f03b7edbbe1905aa5781f149d82",
     "grade": false,
     "grade_id": "cell-8f226a41e9f4a6b6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2456d61167d6bf5dcfa6d2281671a22",
     "grade": false,
     "grade_id": "cell-5356a64b4c15ed52",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Vamos a recordar el modelo de la regresión logística. Es el mismo que para la regresión lineal pero aplicando una función sigmoide sobre el resultado para conseguir una función convexa y, de esta forma, poder aplicar el descenso por gradiente para minimizar la función de coste.\n",
    "\n",
    "#### Hipótesis de la regresión logística\n",
    "#### $$ h_{\\theta}(x) = g(\\theta^{T}x)$$\n",
    "#### La función $g$ es la función sigmoide definida como\n",
    "#### $$ g(z)=\\frac{1}{1+e^{−z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a75de03aa9a0fa38cb5312975ef53299",
     "grade": false,
     "grade_id": "cell-dc50c2dddf5fe33e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Definir la función que aplica la función sigmoide, $g(z)$, sobre un dato $z$. Este dato puede ser un array de valores.\n",
    "\n",
    "La función exp de numpy (pasando un valor como parámetro de entrada) sirve para calular $e^{valor}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb7f41a762fefdc2fe4f8a271ce908b0",
     "grade": false,
     "grade_id": "cell-a04357a8872ab2ed",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "#     return(#<RELLENAR>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4bdd9b9e1fdb25f1cdf804a241fc312e",
     "grade": false,
     "grade_id": "cell-43add526e6327512",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Comprobar si la función sigmoide funciona correctamente, recordar que:\n",
    "* Para el valor 0.0 el resultado debe ser 0.5\n",
    "* Para valores grandes el resultado debe ser muy próximo a 1.0\n",
    "* Para valores pequeños el resultado debe ser muy próximo a 0.0\n",
    "\n",
    "Ejericicio:\n",
    "* Comprobar el resultado para el valor 0.0\n",
    "* Comprobar el resultado para un array de numpy compuesto por los valores -5.0 y 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62b326c7e5596bde4b4cd80923820d78",
     "grade": false,
     "grade_id": "cell-edc57fb7a385b5ce",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# resultado1 = #<RELLENAR>\n",
    "\n",
    "\n",
    "print(resultado1)\n",
    "# resultado2 = #<RELLENAR>\n",
    "\n",
    "\n",
    "print(resultado2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e716591b4e3fff68dd28bf23ffa034ff",
     "grade": true,
     "grade_id": "cell-c4b7abd08f24d17d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# ESTA CELDA DARÁ ERROR SI EL RESULTADO NO ES CORRECTO\n",
    "    # EN CASO CONTRARIO NO TENDRÁ SALIDA\n",
    "assert_equal(resultado1, 0.5)\n",
    "assert_equal(list(map(lambda ind: round(ind, 5), list(resultado2.ravel()))), [0.00669,  0.99331])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4bd1ccc3ae42cfb10823e3fdcb3becf",
     "grade": false,
     "grade_id": "cell-751391b0cf30baf1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "La librería Scipy contiene una función que realiza exáctamente este mismo cálculo. Se puede consultar su información la siguiente URL:<BR>\n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html#scipy.special.expit\n",
    "\n",
    "Se podría utilizar en lugar de la función sigmoide que acabamos de definir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e588995b331abf6390a90c6c5380a12b",
     "grade": false,
     "grade_id": "cell-732ee2e86dcc4bc9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "A continuación vamos a implementar la función de coste. Vamos a recordar su expresión normal y vectorizada.\n",
    "\n",
    "#### Función de coste\n",
    "#### $$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big[-y^{(i)}\\, log\\,( h_\\theta\\,(x^{(i)}))-(1-y^{(i)})\\,log\\,(1-h_\\theta(x^{(i)}))\\big]$$\n",
    "#### Función de coste vectorizada\n",
    "#### $$ J(\\theta) = -\\frac{1}{m}\\big((\\,log\\,(g(X\\theta))^Ty+(\\,log\\,(1-g(X\\theta))^T(1-y)\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8855ee9a75d4f86527a47ced4a384ce0",
     "grade": false,
     "grade_id": "cell-1992df27a94abf33",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Definir la función de coste vectorizada mostrada anteriormente $J(\\theta)$. Esta función recibe como parámetros de entrada los valores de los parámetros $\\theta$ sobre los que calcular el coste con respecto a los datos de entrada (X) y sus correspondientes salidas (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35a1a3b7bd9c7e8d2d02b49ffb1b5e18",
     "grade": false,
     "grade_id": "cell-9dc9ad65f52a1ab1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def funcionCoste(theta, X, y):\n",
    "    m = y.size\n",
    "    h = sigmoide(X.dot(theta))\n",
    "    \n",
    "    J = -1.0*(1.0/m)*(np.log(h).T.dot(y)+np.log(1.0-h).T.dot(1.0-y))\n",
    "               \n",
    "    if np.isnan(J[0]):\n",
    "        return(np.inf)\n",
    "    return(J[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a6792593aa7a47877124a193edf6130",
     "grade": false,
     "grade_id": "cell-e75003a6f0754465",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Recordamos como son las derivadas parciales para poder definir la función que aplica el descenso por gradiente y de esta forma aprender los valores de los parámetros de la regresión logística ($\\theta$)\n",
    "\n",
    "#### Derivada parcial\n",
    "\n",
    "#### $$ \\frac{\\partial J(\\theta)}{\\partial\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j} $$ \n",
    "#### Derivada parcial vectorizada\n",
    "#### $$ \\frac{\\partial J(\\theta)}{\\partial\\theta_{j}} = \\frac{1}{m} X^T(g(X\\theta)-y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e30b45b741d98714e98006ce16f5eb26",
     "grade": false,
     "grade_id": "cell-1267b93973379b0c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Definir la función que realiza la derivada parcial vectorizada mostrada previamente. Esta función recibe como parámetros de entrada los valores iniciales de los parámetros de la regresión logística ($\\theta$) así como los valores de las variables de entrada del problema (X) y sus correspondientes salidas (y).\n",
    "\n",
    "En la variable h tenemos el resultado de $g(X\\theta)$\n",
    "\n",
    "Ejercicio: \n",
    "* Implementa la ecuación de la derivada parcial $\\frac{1}{m} X^T(g(X\\theta)-y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaa8e646f7a81e8a38a0bc59765f549e",
     "grade": false,
     "grade_id": "cell-c602192ee8ff954f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def derivadaParcial(theta, X, y):\n",
    "    m = y.size\n",
    "    h = sigmoide(X.dot(theta.reshape(-1,1)))\n",
    "    \n",
    "#     derParcial = #<RELLENAR>\n",
    "\n",
    "\n",
    "\n",
    "    return(derParcial.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abc27e8747b11d78535f173cd68fc073",
     "grade": false,
     "grade_id": "cell-b0c5b6cadf596e0a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Ejercicio:\n",
    "* Crear los valores iniciales de los parámetros de la regresión logística como un vector columna de ceros. \n",
    "* Comprobar el coste del modelo con los datos leídos utilizando estos valores iniciales de los parámetros $\\theta$.\n",
    "* Obtener los valores de la derivada parcial de los parámetros de la regresión logística llamando a la función que la realiza utilizando los datos de entrenamiento leídos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12fbbf0a041e943bef95fcbd4fd8e0b1",
     "grade": false,
     "grade_id": "cell-26477f23b9b5051e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Creación de la columna de ceros\n",
    "# thetaInicial = #<RELLENAR>\n",
    "\n",
    "\n",
    "# Llamada a la funcion de calculo del coste\n",
    "# costeInicial = #<RELLENAR>\n",
    "\n",
    "\n",
    "# Llamada a la funcion que realiza la derivada parcial\n",
    "# derParcial = #<RELLENAR>\n",
    "\n",
    "print('Coste del modelo inicial: \\n', costeInicial)\n",
    "print('Derivada parcial: \\n', derParcial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c50dbbb50532016b776272ef9e2acfa",
     "grade": true,
     "grade_id": "cell-3a7fb6b5c0c804c8",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# ESTA CELDA DARÁ ERROR SI EL RESULTADO NO ES CORRECTO\n",
    "    # EN CASO CONTRARIO NO TENDRÁ SALIDA\n",
    "assert_equal(round(costeInicial, 5), 0.69315)\n",
    "assert_equal(list(map(lambda ind: round(ind, 5), list(derParcial.ravel()))), [-0.1,  -12.00922, -11.26284])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d699501803655845f8a2e7092aedc40",
     "grade": false,
     "grade_id": "cell-609baf42836342e9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Optimización de la función de coste\n",
    "\n",
    "Para realizar el aprendizaje de los parámetros de la regresión logística vamos a utilizar la función minimize importada al comienzo de la práctica. Esta función realiza el proceso de descenso por gradiente y recibe como parámetros de entrada:\n",
    "* La función que calcula el coste del modelo\n",
    "* Los valores iniciales de los parámetros del modelo (variable thetaInicial)\n",
    "* Los valores de los ejemplos de entrenamiento (tanto de las variables de entrada (X) como de salida (y)) en forma de tupla (parámetro args)\n",
    "* La función que realiza las derivadas parciales (parámetro jac)\n",
    "* El número máximo de iteraciones del descenso por gradiente (establecido a 400)\n",
    "\n",
    "Los parámetros del modelo aprendidos ($\\theta$) se devuelven en el campo x de la variable utilizada para recoger la salida de la función minimize. En este caso se pueden consultar imprimiedo res.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34d7cdc441e2a8872ce741f8be410db9",
     "grade": false,
     "grade_id": "cell-481a267eaea273a6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "res = minimize(funcionCoste, thetaInicial, args=(X,y), jac=derivadaParcial, options={'maxiter':400})\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c9efb002da6cc45813f7ff71befcbdb",
     "grade": true,
     "grade_id": "cell-4d84bebdfd144e18",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# ESTA CELDA DARÁ ERROR SI EL RESULTADO NO ES CORRECTO\n",
    "    # EN CASO CONTRARIO NO TENDRÁ SALIDA\n",
    "assert_equal(list(map(lambda ind: round(ind, 5), list(res.x.ravel()))), [-25.16132,  0.20623, 0.20147])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfa39d45d8afde6504d770748c10c3be",
     "grade": false,
     "grade_id": "cell-d17b377b338e437f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Predicción de nuevos datos\n",
    "\n",
    "Para predecir la clase de nuevos estudiantes se debe definir una función que recibe los valores de los parámetros del modelo ($\\theta$), una lista de lista con los valores de las variables de entrada de todos los alumnos (X) y un umbral para decidir si se predice la clase admitido o no admitido. La función sigmoide definida anteriormente devuelve un valor en el intervalo [0, 1] que debemos transformar a 0 o 1. Para realizar este proceso utilizamos el umbral de tal forma que si el valor devuelto por la función sigmoide es menor que el umbral se obtiene False (0) y en otro caso True (1). Por tanto, este umbral por defecto debe ser 0.5 para que ambas clases tengan la misma probabilidad de ser predichas.\n",
    "\n",
    "Ejercicio:\n",
    "* Calcular la probabilidad de admisión de todos los alumnos (función sigmoide pasando el resultado de la ecuación del modelo $X\\theta^{T}$). \n",
    "    * Se asigna a la variable p.\n",
    "* Comparar cada valor devuelto por la función sigomide (almacenado en p) con el umbral para obtener si son admitidos (mayor o igual que el umbral: True) o no (menor que el umbral: False). \n",
    "    * La variable p se actualiza con este resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "280051138e8a56cafde4920ba4cc78c2",
     "grade": false,
     "grade_id": "cell-10c82245e36cda8a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def prediccion(theta, X, umbral=0.5):\n",
    "#     p = #<RELLENAR>\n",
    "\n",
    "    # La función astype la utilizamos para convertir booleanos (False (0), True (1)) en números enteros {0, 1}.\n",
    "    return(p.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1e9b8d9fc3de2d06f8c6e6e419af28a",
     "grade": false,
     "grade_id": "cell-09582e3253035e30",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Calcular la probabilidad de admisión de un estudiante que ha obtenido un 45 en el primer examen y un 85 en el segundo. Para ello se debe llamar a la función sigmoide ya que nos devuelve la probabilidad de que un estudiante sea admitido (número entre 0 y 1 como se ha mencionado en el apartado previo).\n",
    "\n",
    "Ejercicio:\n",
    "* Definir el array de numpy con los datos del alumno para que pueda ser predicho por el modelo (acordaros de incluir el 1).\n",
    "\n",
    "Nota:\n",
    "* Fijaros que a la sigmoide se le pasa el alumno (X) multiplicado por los parámetros del modelo transpuestos (res.x.T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f12449919d9f77eb3411cd6bd0f111e9",
     "grade": false,
     "grade_id": "cell-55b696c5195f0255",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Predecir utilizando los theta obtenidos tras aplicar la función minimize (res.x)\n",
    "# alumno = #<RELLENAR>\n",
    "\n",
    "probabilidad = sigmoide(alumno.dot(res.x.T))\n",
    "print(probabilidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cd51b3ab591ec420dbbceea946eaa77",
     "grade": true,
     "grade_id": "cell-fedf000a263c4744",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# ESTA CELDA DARÁ ERROR SI EL RESULTADO NO ES CORRECTO\n",
    "    # EN CASO CONTRARIO NO TENDRÁ SALIDA\n",
    "assert_equal(round(probabilidad, 5), 0.77629)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90bd141e0036eea2581ab4e81a65a05f",
     "grade": false,
     "grade_id": "cell-4b1e811d5a004cc9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Ejercicio:\n",
    "* Realizar la predicción de todos los alumnos almacenados en el fichero que contiene el histórico (el leído al comienzo de la práctica). \n",
    "    * Llamar a la función predicción definida anteriormente pasando los valores de los parámetros del modelo (res.x) y los datos a predecir.\n",
    "* Obtener el porcentaje de acierto aplicando los siguientes pasos:\n",
    "    * Obtener un array de booleanos comprobando si los alumnos se predicen correctamente o no (p==y.ravel())\n",
    "    * Obtener el número de alumnos correctamente predichos, sumar el array anterior (con la función sum se suman los True ya que son unos)\n",
    "    * Obtener el porcentaje de alumnos correctamente predichos divididiendo la suma anterior entre el número total de alumnos (p.size) y multiplicado por 100. \n",
    "        * Este resultado se almacena en la variable accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21d0ff5e7a2a887ae6f53e6877b0a4bb",
     "grade": false,
     "grade_id": "cell-a6aa483bab2ca6fe",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# p = #<RELLENAR>\n",
    "# accuracy = #<RELLENAR>\n",
    "\n",
    "print('Precisión en entrenamiento {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a71b2ec1891721159685d827041d13ee",
     "grade": true,
     "grade_id": "cell-d8c8b0b6ba86795f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# ESTA CELDA DARÁ ERROR SI EL RESULTADO NO ES CORRECTO\n",
    "    # EN CASO CONTRARIO NO TENDRÁ SALIDA\n",
    "assert_equal(round(accuracy, 1), 89.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d24519b1b69fd40405db2587e963f7c",
     "grade": false,
     "grade_id": "cell-ba987086ceb8c733",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Frontera de decisión\n",
    "\n",
    "Ahora vamos a mostrar la frontera de decisión que genera el modelo de regresión logística que acabamos de aprender. Recordar que la frontera de decisión se genera en el punto en el que las probabilidades del ejemplo a todas las clases son iguales y, por tanto, no se sabe determinar la clase. A partir de ese punto la clase cambia entre las que estén afectadas por la frontera (generalmente dos).\n",
    "\n",
    "NOTA:\n",
    "* Fijaros en como se obtiene la frontera de decisión (línea azul): se aplica la sigmoide (probabilidad de cada alumno) y se muestran solamente los alumnos cuya probabilidad sea 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fa3a32e2e737f9ee81723c3fa04186d",
     "grade": false,
     "grade_id": "cell-d3581e8b66064284",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Mostramos el punto que hemos clasificado anteriormente como un triángulo rojo\n",
    "plt.scatter(45, 85, s=60, c='r', marker='v', label='(45, 85)')\n",
    "muestraGrafica(datos, 'Nota en el examen 1', 'Nota en el examen 2', 'Admitido', 'No admitido')\n",
    "# Calculamos el mínimo y máximo de cada variable de entrada\n",
    "x1_min, x1_max = X[:,1].min(), X[:,1].max(),\n",
    "x2_min, x2_max = X[:,2].min(), X[:,2].max(),\n",
    "# Obtenemos todas las combinaciones de valores de las dos variables (comprendidos entre el mínimo y el máximo y espaciados linealmente)\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "# Aplicamos la función sigmoide para obtener la probabilidad de admisión de todas las combinaciones anteriores\n",
    "h = sigmoide(np.c_[np.ones((xx1.ravel().shape[0],1)), xx1.ravel(), xx2.ravel()].dot(res.x))\n",
    "h = h.reshape(xx1.shape)\n",
    "# Mostramos con una línea azul todos las combinaciones cuya probabilidad de admisión sea 0.5 (equiprobable: no sabemos elegir)\n",
    "plt.contour(xx1, xx2, h, [0.5], linewidths=1, colors='b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25efb4ce63ec3b6de6c17de0823edbee",
     "grade": false,
     "grade_id": "cell-6a64aa35faca3c46",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Regresión logística con regularización\n",
    "\n",
    "En la segunda parte de la práctica vamos a trabajar con un nuevo problema de clasificación. En este caso se trata de predecir si un microchip pasa los tests de calidad o no. Para ello, a cada microchip se le realizan dos tests, cuyos resultados se almacenan en un archivo que contiene los datos históricos de 118 microchips. Por tanto, para cada microchip se tienen dos variables de entrada (los resultados de los dos tests) y la variable de salida que tiene dos valores: pasa el test de calidad, son aceptados, (1) o no, son rechazados, (0). Los datos de este problema están almacenados en el fichero llamado ex3data2.txt cuyo delimitador también es la coma (,).\n",
    "\n",
    "Realiza la llamada a la función que realiza la lectura de los ficheros y almacena los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7045c04b4b6ddd6e42f769c6d4513550",
     "grade": false,
     "grade_id": "cell-0a0c26184f0530fc",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# data2 = #<RELLENAR>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20e5c46efdf4d55ebf3ea8b885706a22",
     "grade": false,
     "grade_id": "cell-6802c11204c06030",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Crea las variables de entrada (X) y de salida (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05006b22cf2e69a133aba7896f5bc08f",
     "grade": false,
     "grade_id": "cell-3640287e9fced594",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# X = #<RELLENAR>\n",
    "# y = #<RELLENAR>\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f89348987595aa2f3351212265efdc1c",
     "grade": false,
     "grade_id": "cell-d62cb158effdca66",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Llama a la función que muestra la gráfica con los ejemplos del histórico de microchips y nombra correctamente los ejes y lo títulos de la leyenda. La clase positiva (1) son los microchips aceptados y la clase negativa (0) los rechazados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05c149ceadc32dcd61a67d6dfdee3dd1",
     "grade": false,
     "grade_id": "cell-2f2e52c6becfbd24",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc167358b251a83f353298df398b18d1",
     "grade": false,
     "grade_id": "cell-66c584e42655c051",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Polinomios\n",
    "\n",
    "Como podemos ver en la gráfica anterior los ejemplos no son linealmente separables. Por este motivo, en primer lugar, vamos a crear nuevas variables del modelo que nos permitan obtener fronteras de decisión no lineales. Para ello\n",
    "* Aplicar la función PolynomialFeatures pasando como parámetro de entrada el valor 6 ya que vamos a generar todos los términos polinómicos de x1 y x2 hasta la potencia 6: en total hay 28 combinaciones (por tanto se generan 28 variables).\n",
    "* Una vez creado el objeto que nos permite realizar dicha transformación llamamos a la función fit_transform utilizando como parámetro de entrada las variables de entrada de nuestro problema.\n",
    "* Esta función tiene dos partes (fit y transform)\n",
    "    * La parte fit aprende los polinomios para generar las nuevas variables\n",
    "    * La parte transform aplica los polinomios generados para transformar los datos de entrada y que como consecuencia pasemos de tener dos variables de entrada a 28.\n",
    "\n",
    "NOTA: los términos polinómicos de las variables a y b hasta la potencia p=3 serían: 1, a, b, $a^2$, $b^2$, $a^3$, $b^3$, ab, a$b^2$, $a^2$$b^2$\n",
    "* Es decir, cada variable se eleva a todas las potencias entre 0 y p, y las combinaciones entre ellas no incluyen lostérminos elevados a p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a128b8b1da52d09f03bb8bcca51b63ca",
     "grade": false,
     "grade_id": "cell-17f69fceaaee8117",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(6)\n",
    "XX = poly.fit_transform(data2[:,0:-1])\n",
    "print(XX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca9048f8f3e43710ca1fdc43a2d03829",
     "grade": false,
     "grade_id": "cell-78a478eaeb2b5eb5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Función de coste regularizada\n",
    "* Observar que se introduce el sumatorio de los parámetros $\\theta$ ponderado por el valor del parámetro $\\lambda$\n",
    "    * Si $\\lambda=0$ obtenemos la función de coste implementada en la primera parte de la práctica\n",
    "    * Si $\\lambda>0$ la función de coste trata de disminuir el valor de los parámetros $\\theta$ del modelo\n",
    "        * A mayor valor de $\\lambda$ menores valores de los parámetros $\\theta$ obtenedremos ya que trataremos de minimizarlos\n",
    "    \n",
    "#### $$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big[-y^{(i)}\\, log\\,( h_\\theta\\,(x^{(i)}))-(1-y^{(i)})\\,log\\,(1-h_\\theta(x^{(i)}))\\big] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$$\n",
    "#### Función de coste vectorizada\n",
    "#### $$ J(\\theta) = \\frac{1}{m}\\big((\\,log\\,(g(X\\theta))^Ty+(\\,log\\,(1-g(X\\theta))^T(1-y)\\big) + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "159839b4327a2ed95af594f4a90c1ef5",
     "grade": false,
     "grade_id": "cell-eaa9fd7daf8064bd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Definir la función de coste que inluye la regularización. Para ello se debe seguir usando la función sigmoide definida en la primera parte de la práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6122e6c8e0af28654ba1bf504e5a8117",
     "grade": false,
     "grade_id": "cell-ba1892b8873dc661",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def funcionCosteRegularizada(theta, valorLambda, XX, y):\n",
    "    m = y.size\n",
    "    h = sigmoide(XX.dot(theta))\n",
    "    \n",
    "    J = -1.0*(1.0/m)*(np.log(h).T.dot(y)+np.log(1-h).T.dot(1-y)) + (valorLambda/(2.0*m))*np.sum(np.square(theta[1:]))\n",
    "    \n",
    "    if np.isnan(J[0]):\n",
    "        return(np.inf)\n",
    "    return(J[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2574d43bf0a9daa116242e135abab5e",
     "grade": false,
     "grade_id": "cell-fa0093e6bad23a41",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Recordamos las derivadas parciales de la regresión logística regularizada\n",
    "\n",
    "#### Derivada parcial\n",
    "\n",
    "#### $$ \\frac{\\partial J(\\theta)}{\\partial\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j} + \\frac{\\lambda}{m}\\theta_{j}$$ \n",
    "####  Derivada parcial vectorizada\n",
    "#### $$ \\frac{\\partial J(\\theta)}{\\partial\\theta_{j}} = \\frac{1}{m} X^T(g(X\\theta)-y) + \\frac{\\lambda}{m}\\theta_{j}$$\n",
    "Nota: el valor del término independiente $\\theta_{0}$ no debe ser normalizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f882a930b4ce9f0bb68da3ca7281ef6",
     "grade": false,
     "grade_id": "cell-612d8746580493f4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Definir la función que realiza la derivada parcial de los parámetros de la regresión logística regularizada en base al error cometido. Para ello se debe aplicar la ecuación de la derivada parcial vectorizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8c5f191cda85b67b9cded63abca1566",
     "grade": false,
     "grade_id": "cell-b7854dfcddb81e3c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def derivadaParcialRegularizada(theta, valorLambda, XX, y):\n",
    "    m = y.size\n",
    "    h = sigmoide(XX.dot(theta.reshape(-1,1)))\n",
    "      \n",
    "    derParcial = (1.0/m)*XX.T.dot(h-y) + (valorLambda/m)*np.r_[[[0]],theta[1:].reshape(-1,1)]\n",
    "        \n",
    "    return(derParcial.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9c816e37a1efa4829b5fde9751aac0f",
     "grade": false,
     "grade_id": "cell-43380ece546928ff",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Ejercicio:\n",
    "* Inicializa los valores iniciales de la regresión logística a una columna de ceros\n",
    "* Llama a la función que calcula el coste del modelo con los datos transformados (XX) mediante los polinomios aprendidos previamente y el parámetro $\\lambda=1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51291ef76e58bbc576df9d1fe6d07225",
     "grade": false,
     "grade_id": "cell-51a12fa38d5c491a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Columna de ceros como valores inciales de los parametros del modelo\n",
    "# thetaInicial = #<RELLENAR>\n",
    "\n",
    "\n",
    "# Llamada a la funcion que calcula el coste del modelo inicial\n",
    "# coste = #<RELLENAR>\n",
    "\n",
    "print(coste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6102d48ebd1489148a2655399d4f8d04",
     "grade": true,
     "grade_id": "cell-b04be24d707c9d2c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# ESTA CELDA DARÁ ERROR SI EL RESULTADO NO ES CORRECTO\n",
    "    # EN CASO CONTRARIO NO TENDRÁ SALIDA\n",
    "assert_equal(round(coste, 5), 0.69315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02da44ce9f79e59c8b0bcc9ff0399067",
     "grade": false,
     "grade_id": "cell-65a312ccb6077efd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Por último vamos a analizar el efecto del parámetro $\\lambda$ en los resultados obtenidos. Para ello vamos a analizar 3 valores: 0, 1 y 100. Para cada valor hay que realizar el siguiente proceso:\n",
    "* Llamar a la función minimize (descenso por gradiente) pasando como argumentos de entrada:\n",
    "    * La función que calcula el coste del modelo\n",
    "    * Los valores iniciales del modelo que acabamos de asignar\n",
    "    * Una tupla que contiene: el valor de $\\lambda$, los datos transformados con los polinomios (XX) y las clases de los 118 ejemplos (parámetro args)\n",
    "    * La función que realiza la derivada parcial de los parámetros del modelo (parámetro jac)\n",
    "    * El número máximo de iteraciones que lo fijamos a 3000\n",
    "* Realizar la predicción de los ejemplos del histórico (datos guardados en XX) con el modelo aprendido (res2.x) (llamada a la función prediccion definida anteriormente)\n",
    "* Calcular el porcentaje de acierto del modelo (se calcula como se ha realizado en la primera parte de la práctica)\n",
    "* Mostrar la gráfica con la frontera de decisión generada por el modelo aprendido (llamada a la función que muestra los datos y sobre la cual mostramos la frontera de decisión generada)\n",
    "    * Como caption mostramos el valor de $\\lambda$, el porcentaje de acierto\n",
    "* Mostrar los valores de los parámetros del modelo\n",
    "\n",
    "Podéis observar los siguientes hechos:\n",
    "* Con $\\lambda=0$ el modelo obtenido es muy complejo puesto que está muy ajustado a los datos con los que ha sido aprendido. El porcentaje de acierto es muy alto (91.53%) y los valores de los parámetros $\\theta$ son muy altos\n",
    "* Con $\\lambda=1$ el modelo obtenido es más sencillo y ofrece un buen balance entre complejidad y porcentaje de acierto (83.05%). Los valores de los parámetros $\\theta$ son más bajos que antes.\n",
    "* Con $\\lambda=100$ el modelo obtenido es muy sencillo pero no se ajusta bien a los datos de entrenamiento y por tanto se obtiene un porcentaje de acierto muy bajo para este problema (61.02%). Los valores de los parámetros $\\theta$ son muy bajos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17e5ba241515065dfccdcb23c9cbceed",
     "grade": false,
     "grade_id": "cell-7fac546456c2f4be",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, sharey = True, figsize=(17,5))\n",
    "\n",
    "# Fronteras de decisión\n",
    "# Lambda = 0 : No se realiza regularización --> modelo muy flexible: sobre-entrenado\n",
    "# Lambda = 1 : Parece que es un buen modelo: simple y preciso\n",
    "# Lambda = 100 : Demasiada regularización: mucho bías\n",
    "\n",
    "for i, valorLambda in enumerate([0.0, 1.0, 100.0]):\n",
    "    # Optimización de los parámetros del modelo en base al coste\n",
    "    res2 = minimize(funcionCosteRegularizada, thetaInicial, args=(valorLambda, XX, y), jac=derivadaParcialRegularizada, options={'maxiter':3000})\n",
    "    \n",
    "    # Obtener las predicciones del modelo (res2.x) para los datos transformados con los polinomios (XX)\n",
    "#     predicciones = #<RELLENAR>\n",
    "\n",
    "    # Calcular el porcentaje de acierto como hemos realizado en la primera parte de la práctica\n",
    "#     accuracy = #<RELLENAR>\n",
    "\n",
    "    # Gráfica que muestra los ejemplos\n",
    "    muestraGrafica(data2, 'Test 1 del microchip', 'Test 2 del microchip', 'Aceptado', 'Rechazado', axes.flatten()[i])\n",
    "    \n",
    "    # Mostramos la frontera de decisión\n",
    "    x1_min, x1_max = X[:,0].min(), X[:,0].max(),\n",
    "    x2_min, x2_max = X[:,1].min(), X[:,1].max(),\n",
    "    print(x1_min, x1_max)\n",
    "    print(x2_min, x2_max)\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "#     print np.c_[xx1.ravel(), xx2.ravel()][:3]\n",
    "#     print poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]).dot(res2.x)[:5]\n",
    "    h = sigmoide(poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]).dot(res2.x))\n",
    "    h = h.reshape(xx1.shape)\n",
    "#     print (h>0.49).sum()\n",
    "        \n",
    "    axes.flatten()[i].contour(xx1, xx2, h, [0.5], linewidths=1, colors='g');       \n",
    "    axes.flatten()[i].set_title('Porcentaje de acierto {}% con Lambda = {}'.format(np.round(accuracy, decimals=2), valorLambda))\n",
    "    \n",
    "    print('Valores de theta para lambda {}: {}'.format(valorLambda, res2.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37machlearn] *",
   "language": "python",
   "name": "conda-env-py37machlearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
